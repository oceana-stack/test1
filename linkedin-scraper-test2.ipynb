{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6c4eb0-a597-436b-a391-50563b91d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 profiles\n",
      "✅ Done Scraping by Past Company!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def chrome(headless=False):\n",
    "    opt = Options()\n",
    "    if headless:\n",
    "        opt.add_argument(\"--headless=new\")\n",
    "    opt.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    opt.add_argument(\"--disable-popup-blocking\")\n",
    "    opt.set_capability(\"goog:loggingPrefs\", {\"performance\": \"ALL\"})\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=opt)\n",
    "    browser.implicitly_wait(10)\n",
    "    return browser\n",
    "\n",
    "# Launch browser\n",
    "browser = chrome(True)  # set True for headless\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "browser.implicitly_wait(3)\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "elementID = browser.find_element(By.ID, \"username\")\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element(By.ID, \"password\")\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "\n",
    "# ===========================\n",
    "# SEARCH PEOPLE BY PAST COMPANY\n",
    "# ===========================\n",
    "\n",
    "companies = {\n",
    "    \"Google\": \"1441\",   # Example: LinkedIn's internal company ID for Google\n",
    "    \"Microsoft\": \"1035\"\n",
    "}\n",
    "\n",
    "profile_links = []\n",
    "\n",
    "for company, comp_id in companies.items():\n",
    "    search_url = f\"https://www.linkedin.com/search/results/people/?facetPastCompany=%5B%22{comp_id}%22%5D\"\n",
    "    browser.get(search_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll to load more results\n",
    "    for _ in range(3):  \n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Parse search results\n",
    "    soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "    anchors = soup.find_all(\"a\", {\"class\": \"app-aware-link\"}, href=True)\n",
    "\n",
    "    for a in anchors:\n",
    "        href = a[\"href\"]\n",
    "        if \"/in/\" in href and href not in profile_links:\n",
    "            profile_links.append(href.split(\"?\")[0])  # clean query string\n",
    "\n",
    "print(f\"Found {len(profile_links)} profiles\")\n",
    "\n",
    "# ===========================\n",
    "# SCRAPE PROFILE DETAILS\n",
    "# ===========================\n",
    "\n",
    "info = []\n",
    "\n",
    "for link in profile_links[:5]:  # limit for demo\n",
    "    browser.get(link)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        name = soup.find(\"h1\").get_text().strip()\n",
    "    except:\n",
    "        name = None\n",
    "\n",
    "    try:\n",
    "        title = soup.find(\"div\", {\"class\": \"text-body-medium\"}).get_text().strip()\n",
    "    except:\n",
    "        title = None\n",
    "\n",
    "    try:\n",
    "        location = soup.find(\"span\", {\"class\": \"text-body-small\"}).get_text().strip()\n",
    "    except:\n",
    "        location = None\n",
    "\n",
    "    info.append([name, title, location, link])\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(info, columns=[\"Full Name\", \"Title\", \"Location\", \"Profile URL\"])\n",
    "df.to_csv(\"past_company_people.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done Scraping by Past Company!\")\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977191ee-2089-4478-8202-c1fe25b90953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc5e6c-3f4c-4cc5-bb00-794aff2fb0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
